{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import functools\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import itertools as it\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pickle\n",
    "import codecs\n",
    "from spacy.pipeline import Pipe\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Source: https://stackoverflow.com/questions/21708192/how-do-i-use-the-json-module-to-read-in-one-json-object-at-a-time/21709058\n",
    "Author: unutbu\n",
    "Purpose: Read in 1 JSON object at a time into a list\n",
    "'''\n",
    "def json_parse(fileobj, decoder=json.JSONDecoder(), buffersize=2048, \n",
    "               delimiters=None):\n",
    "    remainder = ''\n",
    "    for chunk in iter(functools.partial(fileobj.read, buffersize), ''):\n",
    "        remainder += chunk\n",
    "        while remainder:\n",
    "            try:\n",
    "                stripped = remainder.strip(delimiters)\n",
    "                result, index = decoder.raw_decode(stripped)\n",
    "                yield result\n",
    "                remainder = stripped[index:]\n",
    "            except ValueError:\n",
    "                # Not enough data to decode, read more\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/tweets.txt', 'r') as rf:\n",
    "    json_list = list(json_parse(rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12652.916 USD  https://t.co/3T5IP5YsEn | #btc #bitcoin #blockchain #cryptocurrency https://t.co/eB0Ji7u78N\n",
      "\n",
      "Something to consider if you suffer from #fomo over bitcoin: you might make or lose money by investing now, but you won‚Äôt lose anything by NOT investing. \n",
      "\n",
      "Few great quotes in this article: https://t.co/S910SsTFDX\n",
      "\n",
      "12660.50 USD  https://t.co/vmknxeCRiL | #btc #bitcoin #blockchain #cryptocurrency https://t.co/a186DDXLAU\n",
      "\n",
      "Lightning Network May Not Solve Bitcoin's Scaling 'Trilemma' https://t.co/umtLQyur31 #bitcoin #blockchain Via‚Ä¶ https://t.co/tCpYmvbDJL\n",
      "\n",
      "MAKE MONEY WITH bitcoin!!!\n",
      "\n",
      "‚úÖKUCOIN is 100% OPEN!!!\n",
      "\n",
      "CLICK ‚ñ∂Ô∏è https://t.co/lPgWGdMR6i\n",
      "\n",
      "‚ùåBinance \n",
      "‚ùåPoloniex \n",
      "‚ùåBittre‚Ä¶ https://t.co/YjUgq2T3mC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(30, 35):\n",
    "    json = json_list[i]\n",
    "    try:\n",
    "        text = json[\"retweeted_status\"][\"extended_tweet\"][\"full_text\"]\n",
    "    except KeyError:\n",
    "        text = json[\"text\"]\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttkzr = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step in our preprocessing is to remove urls and tokenize our tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for json in json_list:\n",
    "    try:\n",
    "        text = json[\"retweeted_status\"][\"extended_tweet\"][\"full_text\"]\n",
    "    except KeyError:\n",
    "        text = json[\"text\"]\n",
    "    text_nourl = re.sub(r\"http\\S+\", \"\", text)\n",
    "    tokens = ttkzr.tokenize(text_nourl)\n",
    "    doc = Doc(nlp.vocab, words=tokens)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JOIN THE LARGEST #BITCOIN #CRYPTO NETWORK ON EARTH ! EMAIL : cryptopennystock@gmail.com #CryptoCurrencyGod #TEAMBILLIONAIRE #PENNYSTOCKS #STOCKALERT #BTC , #ETH , #ADA #XRP #TRX #LTC #BLOCKCHAIN "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_filepath = '../data/docs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(docs_filepath, 'w', encoding='utf_8') as wf:\n",
    "    for doc in docs:\n",
    "        wf.write(doc.text)\n",
    "        wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we lemmatize our tokens and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Source: Modern NLP in Python\n",
    "Author: Patrick Harrison\n",
    "Purpose: Remove punctuations and lemmatize our tokens.\n",
    "'''\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = '../data/unigram_sentences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(unigram_sentences_filepath, 'w', encoding='utf-8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(docs_filepath):\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['join', '-PRON-', 'ico', 'now']\n",
      "['here', 'be', 'a', 'link', 'and', '-PRON-', 'know', 'what', 'to', 'do', 'next', 'üòâ']\n",
      "['become', 'a', 'part', 'of', 'first', 'ecocryptomin', 'in', 'the', 'world']\n",
      "['invest', 'investment', 'innovate', 'innovation', 'mining', 'miningfarm', 'crypto', 'profit', 'bitcoin', 'ethereum', 'cryptocurrency']\n",
      "['st']\n",
      "['james', 'place', 'condos', 'in', 'columbia', 'heights', 'accept', 'bitcoin', 'as', 'payment', 'washington', 'business', 'journal']\n",
      "['rt']\n",
      "['-PRON-', 'wish', 'man', 'would', 'put', 'as', 'much', 'confidence', 'in', 'woman', 'as', '-PRON-', 'do', 'in', 'bitcoin']\n",
      "['there', 'be', 'a', 'direct', 'correlation', 'between', 'the', 'growth', 'in', 'the', 'number', 'of', 'successful', 'and', 'reliable', 'project', 'and', 'the', 'arrival', 'of', 'new', 'investor', 'on', 'the', 'market', 'whose', 'interest', 'will', 'increase', 'significantly']\n",
      "['rr', 'revizorcoin', 'cryptocurrency', 'ico', 'blockchain']\n"
     ]
    }
   ],
   "source": [
    "for sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As people sometimes use the ticker symbol for a coin instead of its commonly used name, we need to go through our tweets and replace all instances of the ticker symbol with its commonly used name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crypto_names = pd.read_csv('../data/crypto-markets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crypto_names = crypto_names[['slug', 'symbol']]\n",
    "crypto_names.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>btc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>ethereum</td>\n",
       "      <td>eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>ripple</td>\n",
       "      <td>xrp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>bitcoin_cash</td>\n",
       "      <td>bch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>cardano</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              slug symbol\n",
       "0          bitcoin    btc\n",
       "1719      ethereum    eth\n",
       "2607        ripple    xrp\n",
       "4228  bitcoin_cash    bch\n",
       "4400       cardano    ada"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_names['symbol'] = crypto_names['symbol'].str.lower()\n",
    "crypto_names['slug'] = crypto_names['slug'].str.replace('-', '_')\n",
    "crypto_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use a dictionary to perform quick lookups and swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crypto_dict = dict(zip(crypto_names['symbol'].values, crypto_names['slug'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_replaced = []\n",
    "for i, sentence in enumerate(unigram_sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        key = word\n",
    "        if key in crypto_dict:\n",
    "            sentence[j] = crypto_dict[key]\n",
    "    unigram_sentences_replaced.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = unigram_sentences_replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we remove stop words from our tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_nostop = []\n",
    "STOP_WORDS.add(\"rt\")\n",
    "for i, sentence in enumerate(unigram_sentences):\n",
    "    sentence_nostop = [token for token in sentence if token not in STOP_WORDS]\n",
    "    sentences_nostop.append(sentence_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = sentences_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['join', '-PRON-', 'ico']\n",
      "['chainlink', '-PRON-', 'know', 'üòâ']\n",
      "['particl', 'ecocryptomin', 'world']\n",
      "['invest', 'investment', 'innovate', 'innovation', 'mining', 'miningfarm', 'crypto', 'profit', 'bitcoin', 'ethereum', 'cryptocurrency']\n",
      "['st']\n",
      "['james', 'place', 'condos', 'columbia', 'heights', 'accept', 'bitcoin', 'payment', 'washington', 'business', 'journal']\n",
      "[]\n",
      "['-PRON-', 'mywish', 'man', 'putincoin', 'confidence', 'woman', '-PRON-', 'bitcoin']\n",
      "['direct', 'correlation', 'growth', 'number', 'successful', 'reliable', 'project', 'arrival', 'new', 'investor', 'market', 'interest', 'increase', 'significantly']\n",
      "['rr', 'revizorcoin', 'cryptocurrency', 'ico', 'blockchain']\n"
     ]
    }
   ],
   "source": [
    "for sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using gensim's Phrases model, we can form bigrams from our raw tokens. For example, [\"big\", \"foot\"] would become \"big_foot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = '../data/bigram_sentences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the detector\n",
    "bigram_model = Phrases(unigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = bigram_model[unigram_sentence]\n",
    "        for word in bigram_sentence:\n",
    "            f.write(word)\n",
    "            f.write(\" \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally we prepare a bag of words (BOW) corpus for LDA topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['otc', 'bitcoin', 'bitcoin', 'crypto', 'blockchain']\n",
      "['-PRON-', 'new', 'ico']\n",
      "['check', 'procedure', 'participate', 'trakinvest', 'ico', 'sale']\n",
      "['participate', '-PRON-', 'presearch_ico', 'sale', 'avail', 'bonus', 'upto', '18', 'cryptocurrency', 'asianico']\n",
      "['trakinvestico', 'ethereum', 'tokensale', 'nextgenaitools']\n",
      "['-PRON-', 'need', 'leave', '-PRON-', 'warm', 'bed', 'late', 'cryptocurrency', 'news']\n",
      "['join', '-PRON-', 'telegram_channel']\n",
      "['jamie', 'dimon', 'buy', 'bitcoin', '2018']\n",
      "['shoe', 'satoshi', 'wear']\n",
      "['find', 'answer', 'bad', 'joke', 'cointelegraph']\n"
     ]
    }
   ],
   "source": [
    "for sentence in it.islice(bigram_sentences, 20, 30):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_filepath = '../data/dictionary.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(bigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "dictionary.compactify()\n",
    "dictionary.save(dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_generator(filepath):    \n",
    "    for tweet in LineSentence(filepath):\n",
    "        yield dictionary.doc2bow(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_filepath = \"../data/bow.mm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize(bow_filepath, bow_generator(bigram_sentences_filepath))\n",
    "bow_corpus = MmCorpus(bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 1.0), (23, 1.0), (41, 3.0), (55, 2.0), (101, 1.0)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[43]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
